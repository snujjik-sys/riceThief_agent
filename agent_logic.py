# -*- coding: utf-8 -*-
"""agent_logic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBd_J6EGs-GAjQKQLOKHziQkSchCiGap

# 1. 준비

## 1.2. OpenAI API 키 설정 및 LLM 초기화
"""

import os
import time
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

API_KEY = os.getenv("OPENAI_API_KEY")

"""## 1.3. 함수"""

def get_agent_components():
    """
    LLM과 Vector DB를 준비해서 반환하는 함수입니다.
    DB 폴더가 없으면 recipes.txt를 이용해 새로 생성합니다.
    """

    # 1. 임베딩 모델 준비
    embed_model = OpenAIEmbeddings(model="text-embedding-3-small")

    db_directory = "./chroma_db"

    # 2. DB 준비
    if os.path.exists(db_directory):
        # (A) DB 폴더가 있으면 -> 바로 로드 (빠름)
        print("[System] 기존 ChromaDB를 로드합니다.")
        db = Chroma(
            persist_directory=db_directory,
            embedding_function=embed_model
        )
    else:
        # DB 폴더가 없으면 -> 텍스트 파일로 새로 생성
        print("[System] DB 폴더가 없어서 recipes.txt로 새로 만듭니다.")
        if os.path.exists("recipes.txt"):
            with open("recipes.txt", "r", encoding="utf-8") as f:
                # 빈 줄 제거하고 리스트로 변환
                recipe_data = [line.strip() for line in f.read().splitlines() if line.strip()]

            db = Chroma.from_texts(
                texts=recipe_data,
                embedding=embed_model,
                persist_directory=db_directory
            )
            print("[System] 새 ChromaDB 생성 완료!")
        else:
            # 파일도 없으면 에러 발생
            raise FileNotFoundError("recipes.txt 파일도 없고 chroma_db 폴더도 없습니다!")

    # 3. LLM 모델 준비
    llm = ChatOpenAI(
        openai_api_key=API_KEY,
        model_name='gpt-5-mini'
    )

    return db, llm

"""# 2. RAG 에이전트 실행 (Multi-turn 구현)"""

def generate_response(db, llm, user_input, chat_history):
    """
    [기능 추가] 사용자 질문과 '대화 기록(history)'을 함께 받아 답변을 생성합니다.
    """

    # 1. 검색 (Retrieval)
    print(f"[System] 검색 중: {user_input}")
    results = db.similarity_search(user_input, k=2)

    if results:
        retrieved_knowledge = "\n".join([x.page_content for x in results])
    else:
        retrieved_knowledge = "관련된 레시피를 찾지 못했습니다."


    # 2. 프롬프트 증강 (Augmentation) - Multi-turn 적용
    # 이전 대화 내용을 텍스트로 예쁘게 정리
    history_text = ""
    if chat_history:
        history_text = "\n".join([f"- {role}: {msg}" for role, msg in chat_history])
    else:
        history_text = "(없음)"

    system_prompt = (
        "너는 자취하는 자식에게 요리를 알려주는 다정한 '엄마'야. "
        "말투는 항상 따뜻하고 친근하게 해줘. "
        "1. [참고 자료]에 있는 레시피를 우선적으로 소개해야 해. "
        "2. [대화 기록]을 읽고 이전 대화의 흐름을 파악해서 자연스럽게 이어가. (예: '아까 말한 계란으로...') "
        "3. 만약 [참고 자료]의 요리가 질문과 관련이 있으면, '엄마 노트에 이런 게 있단다~' 하고 먼저 알려줘."
        "4. 만약 참고 자료에 없으면 '엄마 노트엔 없는데~' 하고 너의 일반적인 지식을 덧붙여서 친절하게 알려줘."
    )

    # 이전 대화 기록을 텍스트로 변환
    history_text = "\n".join([f"{role}: {msg}" for role, msg in chat_history])

    augmented_prompt = f"""
    [참고 자료 - 엄마의 레시피 노트]
    {retrieved_knowledge}

    [대화 기록 (이전 대화 맥락)]
    {history_text}

    [현재 자식의 질문]
    {user_input}

    (위 내용을 종합해서 엄마처럼 대답해줘)
    """

    # 3. 답변 생성 (Generation)
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=augmented_prompt)
    ]

    response = llm.invoke(messages)

    # UI에 표시하기 위해 '답변 내용'과 '참고한 문서 내용'을 같이 반환
    return response.content, retrieved_knowledge